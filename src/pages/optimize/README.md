# 优化

## HTML
### 减少HTML的层级嵌套
- 一个网页或一份`.html`文件对于浏览器来说，实际上仅仅是一串非常长的字符串，对于浏览器来说，它并不在意写的是否优雅
- 浏览器在解析HTML、创建DOM树的过程中，基本会循环下面三个步骤
1. 遇到字符`<`时，状态更改为 `标记打开状态`
2. 当接收到一个`a-z`字符时，会创建起始标记，状态更改为`标记名称状态`，并保持状态到接收`>`字符时，此期间的字符串会形成一个新的标记名称，接收到`>`字符后，将当前的新标记发送给树构造器，状态改回`数据状态`
3. 当接收下一个输入字符`/`时，会创建关闭标记打开状态、并更改为标记名称状态, 当接收`>`字符时，将当前的新标记发送给树构造器，状态改回`数据状态`

- 浏览器在创建解析器的同时，也会创建`document`对象，在DOM树构建阶段，以`document`为根节点的DOM树会不断进行修改，添加各种元素，标记生成器发送的每个节点都会由树构建器进行处理
- 浏览器在解析HTML文件并构建DOM树的过程中，会将我们写的标签项DOM树种挂载，层级越深，DOM树就越深，DOM树在实际的访问中事需要遍历的，举一个简单的例子来说明：
```js
// 一层循环O(n), 会感觉很快
for (let i = 0; i < ele.length; i++) {
    
}

// 二层的循环O(n^2), 速度勉强达标
for (let i = 0; i < ele.length; i++) {
    for (let k = 0; k < ele[i].length; k++) {
        
    }   
}

/*
* 如果出现3层以上，那么此时的速度就变慢了，因为每增加一层，遍历所需要的时间复杂度都呈指数增长
* 虽然浏览器有非常强大的算法优化，但再强大的算法也无法将时间设置为0，也就是说在构建DOM树的过程中，
* 层级越深所需要的时间就越长，计算时所消耗的内存就越大，那么减少标签层级的好处就不言而喻了
* */
```

#### 如果是通过MVVM框架来写组件，那么组件的嵌套和标签嵌套的层数是不是就可以不管了？
```js
function compole(el) {
    let ele = document.querySelector(el) // 获取元素信息
    let fragment = document.createDocumentFragment() // 创建文档对象
    let child
    while(child = ele.firstChild) {
        fragment.appendChild(child)
    }
    ele.appendChild(fragment)
}
/*
* 上述代码是创建一个Fragment(文档碎片),并将取到的DOM元素子节点一个个的网Fragment中堆，直至DOM元素中的子节点全部取完
* 关键是下面这部分，接下来的操作就不是单纯的向数组中堆了，后面通过replace函数将对应的DOM节点与对应数据的 setter 和 getter 绑定并更新替换成数据
* */

if (node.childNodes.length) {
    replace(node)
}
// replace 是一种递归函数，每多一层嵌套，递归的深度就多一层，这样会增加运行时长、内存的占用也会增加
```

### 减少空标签、无用标签
- 假设要做一个形状，用空标签来进行遮挡等，可以通过`:before、:after`伪类完成空标签的功能

### 标签的Style属性
- 标签的`style`属性允许直接在标签上写css,这部分样式的优先级高于ID选择器，但是对于浏览器来说，整个页面的元素在正常情况下是非常多的，而用`style`就会让浏览器要解析的HTML文件从原先的几百B变成了几KB
，甚至是几十KB或更多，文件大小变大了很多，降低了解析的性能，于此同时浏览器还要判断哪些是标签，哪些是CSS样式
- 通过`style`属性在HTML中插入样式，非常不利于维护，也不可复用

### 标签的自定义属性
- 标签的自定义属性应用非常广泛，如`Vue`的`v-model`就是一个自定义属性，这些自定义属性就是用来存放一些临时性的东西, 
如果属性很多，那么`attributes`就会变得很大，单个元素时影响并不大，但是假如页面有非常多的这样的元素就会对性能造成一定的影响，不仅在DOM创建之初需要增加Key和对应的值，后续数组的遍历同样会变慢

#### 比较推荐的做法是，将一个DOM块，比如一个组件单元的顶层设置为自定义属性，因为一般情况下一个后台的请求会对应界面的一块区域显示，这样一个区域作为一个单元，那么自定义属性就不会很多了

### link标签
- 可能大多数人对于link的印象主要停留于外链CSS资源上，比如：
```html
<link rel="stylesheet" href="common.css" >
```
- 现在css引用的方式由于合并的关系，更多的采用了如`@import`的css3用法，较少去看编译后的文件，link更容易被忽视，实际上，link能做的事情比想象中的多，特别是在`prefetch、preload
`等预加载技术出现以后，比如`Vue cli3.0`构建的项目在编译后HTML：
```html
    <!--prefetch 原理是利用浏览器的空闲时间先下载用户指定需要的内容，然后缓存下来，下次再请求就是从缓存中加载-->
    <link href=/css/chunk-f22d7ffe.c9f5d5a6.css rel=prefetch>
    <link href=/js/chunk-03d7688b.1698392b.js rel=prefetch>
    
    <!--preload 预加载，这种异步预先加载的资源暂时不会用到，也就不会执行-->
    <link href=/css/chunk-vendors.a0aaf7b0.css rel=preload as=style>
    <link href=/js/app.aedca39f.js rel=preload as=script>
```

### img标签
- img标签主要用于加载图片资源，在一些网站中，某段时间在某个位置放置了一张图片，这个时间过后图片可能会变动成另外一张，由于浏览器默认的`layout`是文档流，很多时候页面内容的变动都伴随着`Repaint`,
处理不好时还会有`Reflow`
- 假如这个图片通过区域刷新的方式更换了一张图片，而这个图片与原照片不一致(比如未设定好宽高)
，那么浏览器就会根据图片的大小，重新计算页面要展示的位置，就因为这一个小小的改动，基于文档流的后续节点都要跟着调整，最终造成整个页面重拍，这对性能的影响无疑是很大的

#### 解决方案
- 可以给img标签设定一个固定的大小或者外套一个设定好大小的标签，再将图片宽高设置成100%

### src与href属性
- 这两个属性一般都是用于加载资源
1. href表示超文本引用，用在link、a等标签上，表示引用和页面关联，是在当前元素和引用资源之间建立联系
2. src属性表示某个资源的路径，用在`img、script、iframe`上

- 这两个属性有时候会产生问题，比如当属性的值为空时，一些浏览器可能会把当前页面作为属性值加载,这种问题很多时候是很难查找出来的，可能只是感觉页面比较慢
```html
<!--这样写-->
<a href="javasctipt:;"></a>
```



## CSS
### 样式继承与复用
1. 继承 合理利用css继承特性，缩短css资源文件，缩短浏览器的载入时间，提升可读性、降低后期维护难度和后续样式调整时触雷的概率
2. 复用 将两个或多个类名之间的公共样式提取出来

### 尽量避免同一类名的多长渲染
- 比如 A写了一个样式a, 后台公司的B遇到了类似的样式需求，复用了a, 并在私有页面重写了样式a的部分属性
- 那么浏览器是怎么渲染的呢？ 先渲染a的样式，再渲染b的样式，这种问题再项目中是无法避免的，但可以做到尽量减少

### 层级过深的CSS选择器
- 现在基本都是通过预编译器来写css, css预编译器有可以随意嵌套及混合的特点，看个案例：
```scss
//例如：
.container {
    .content {
        .box {
            ul {
                li {
                
                }
            }
        }
    }
}
//那么在编译过后，就是以下这个样子
.container .content .box ul li {

}

```
- 在平时的开发规范中，避免写出层级过深的css选择器经常被提及，这是出于什么原因呢？理论上来说，浏览器会逐级寻找DOM树挂载的样式选择器，并将对应的样式渲染给它，这会对性能产生一定的影响

### 减少昂贵的样式成本
- 尽管CSS相比于JavaScript的性能开销是小巫见大巫，但实际上一些昂贵的样式成本也在不知不觉中消耗了一部分浏览器的性能，而这些在开发当中是可以避免的

#### 何为昂贵的样式
- css
有些样式之所以昂贵，很大的原因在于这部分样式需要显示的方式较为特别，比如过渡、渐变、阴影、圆角等，这部分的样式与一般的大小、颜色样式不同，看似仅仅是一个样式，但是浏览器实际上会将这个样式风格接近的过渡色绘制到页面中，而这些像素级的变换实际上需要大量的计算，虽说显示效果很好，但实际应用中也尽量避免滥用，昂贵的样式有：
1. 绘制阴影：box-shadow
2. 绘制渐变：gradients(有线性渐变和径向渐变)
3. 滤镜：filter
4. 透明度：opacity
5. 圆角：border-radius(浏览器元素实际上都是长方形)

- 这些在PC端(PC端一般都资源充足)还好，在移动端要尽量避免

### 减少浏览器的重拍与重绘
1. 当元素的宽高、位置、字体大小、字体风格等发生改变的时候会发生重排
2. 当元素的外观(color、background、visibility等属性)发生改变时，会触发重绘，重绘是无法避免的，浏览器对此做了优化(将多次的重排、重绘合并为一次执行)
，但还是需要避免不必要的重绘，比如页面滚动时触发的`hover`事件等(可以在页面滚动的时候禁用hover事件)
- 比较常见却容易忽略的有：
1. 改变font-size 和 font-family
2. 改变元素的内外边距
3. 通过JS改变CSS类
4. 通过JS获取DOM元素的位置相关信息(如：width、height、left等)
5. CSS伪类激活
6. 滚动滚动条或改变窗口大小

### CSS 雪碧图
- 如果页面当中有很多小图标，那么每个图标都需要一个请求，假如图标很多，那么请求就会很多，影响整体性能，可以将所有的图标合并成一个图，就可以将多个请求合并成一个，加快页面的响应速度(目前基本很少用了，可以用iconfont或svg替代)

### CSS3动画
- 现在的浏览器对CSS3标准新增的绝大多数属性都是支持的(IE8及以下版本兼容性较低)，如果不需要考虑兼容性，那么就可以使用CSS3了
- CSS3最强大的就是动画的实现，原先的动画都是使用JavaScript，现在很多动画都可以交给CSS3

#### CSS3动画和JavaScript动画实现有什么不同，性能差异在哪里？
- 传统的方式是使用定时器，而CSS3使用的是animation，实现的过程无需采用JavaScript代码，减少JavaScript的权重，利用浏览器对CSS3的无缝支持，即使页面性能得到优化，又使动画变得顺畅
- CSS3对于性能优化来说，通过GPU加速，对动画等方面的影响较为明显，比如JavaScript通过20ms内完成的事件时，CPU可以帮忙减少一部分时间
- 浏览器接收到页面文档后，会将文档中的标记语言解析为DOM树，DOM树和CSS结合后形成渲染树，渲染树包含了大量的渲染元素，每一个渲染元素会被分到一个图层中，每个图层又会被载入GPU形成的渲染纹理中，图层在GPU
中`transform`是不会触发`repaint`的，最终这些使用`transform`中的图层都会由独立的合成器进程进行处理，这样无需过分的渲染就能完成所需的功能，是页面的性能得到提升
- 那么什么是GPU？什么是GPU加速？哪些操作会触发GPU加速？
- GPU可以简单的理解为常说的显卡，最初的计算机是没有图形化界面的，而一个计算机LCD屏上有很多像素级大小的点，最终的图案是由这些点组成的，这涉及巨量的运算，如果都交给CPU，那么CPU的算力负荷将非常高，有了显卡后，CPU
会将图形指令分派给显卡，由专业的显卡来完成这部分计算，显卡本身还可以对图形化界面做进一步的算法优化，而CPU可以完成更重要的系统调度，这就是显卡(GPU)承担的作用，而CSS3的变换属性会触发GPU加速，主要内容有：
1. `translate`：有`translateX、translateY、translate3d`等，主要用于处理图形的平移
2. `rotate`：同样有`x、y、x`轴的旋转
3. `scale`：用于处理图像的缩放
4. `opacity`：用于处理图像的透明度
5. `filter`：滤镜


### media媒体查询
- 在实际开发当中，经常会碰到页面需要响应式，需要自适应屏幕，那么常用的就是 `media` 媒体查询了， 但是其实JavaScript也有一个响应机制`window.resize`,
它在窗口被调整大小时触发，会不断的进行重排和重绘，非常消耗性能，有时会导致类似于死循环的效果，导致页面卡死、崩溃<font color='red'><b>如非必要请不要使用</b></font>


## javasctipt

### 定时器
- 定时器本身是属于JavaScript运行的机制，正常情况下并不会对JavaScript性能造成什么影响，大多数造成的影响是因为使用不当(在定时器内部的逻辑是关于DOM结构，或者一些实现使浏览器过于频繁的重排、重绘时)

#### 及时清除定时器
- 不用的定时器及时清除，避免重复生成

### 合理利用CSS3动画
- JavaScript在动画方面的消耗比较大，可以用CSS3来完成，具体请看上面CSS3动画

### 多利用事件代理委托
- 当绑定一个事件的时候就会开启一个异步监听，等待用户的交互触发、那么如果绑定了大量的事件，会大大影响程序的性能

### 避免重复的事件监听
- 假如在父元素上面做了事件委托，但是又要对里面的某一项进行特殊的单击事件，那么后面的这个监听不会覆盖前面的监听事件，但是这个元素上面的事件会被重复监听两次


## 资源加载
- 网页的性能很多时候其实取决于资源，这里的资源包括媒体资源(图片、音视频)、信息资源、文件、后端服务资源等，在下载资源的时候，资源量越多，所需下载的事件就越长，性能也就越差，原因有两个：
1. 从资源到最终的展示过程需要解析，解析速度越慢，过程的事件也就越长
2. 资源解析完毕后，还需要渲染到页面，渲染时间越长(比如重复渲染)，性能就越差，用户所需等待的事件就越长，用户体验也就越差

### 浏览器请求并发数限制
- 并发是指某一个瞬间运行在同一个处理机或者某个服务器集群上的服务或执行逻辑，比如天猫双十一当天0点刚过的那一瞬间，用户的每一次单击、支付都是一条服务，需要服务端去处理，可以想象一下那时同时过来的请求数，这就是并发。
- 浏览器的请求也是并发进行的，当页面加载时，需要的资源都是从服务端过来的一条条请求，比如页面打开时有5个接口需要返回数据，还有图片、css资源、js资源、音视频资源等，假设这些合起来有20
个请求，实际上浏览器在处理这些请求时并不是一次性将20个请求一起发过去，而是有请求并发限制，减少处理时浏览器本身的线程切换开销

#### 浏览器请求并发数限制的数量
|       版本      |   HTTP1.1 |   HTTP1.0 |     
|   :---:       |   :---:   |   :---:   |
|   IE6,7       |   2       |   4       |
|   IE8         |   6       |   6       |
|   IE9         |   10      |   10      |
|   IE10        |   6       |   6       |
|   IE11        |   6       |   6       |
|   Firxefox    |   6       |   6       |
|   Safari3,4   |   4       |   6       |
|   chrome4+    |   6       |   6       |
|   Opera 10.51+|   8       |   ?       |
|   移动端      |            |         |
|   iphone2,4   |   4       |   ?       |     
|   iphone3,5+  |   6       |   ?       |
|   Android2-4  |   4       |   ?       |
|   Android5+   |   6       |   ?       |     

- 浏览器的请求并发限制在某些场景中影响很大，比如有很多图片的网站，如：京东、淘宝等，资源请求数很容易过百甚至更多

#### 浏览器请求并发限制解决方案
1. 多域名访问，由于浏览器请求的并发限制是针对的是同一个域名下的资源，那么可以将静态资源与服务分离，多域名存储

### DNS优化
- 一个网页从输入一串URL开始到最终呈现，需要经历一个DNS解析的过程，什么是DMS解析呢？

<b>在整个互联网中，每一个计算机都是其中的一个单元节点，有各自的IP，我们平时上网的过程，就是一台计算机访问另一台计算机资源的过程，这时被访问的那台计算机可以称之为服务器，要访问他，就需要知道它的位置，通常情况下是通过IP
+端口来访问的，但是很少有人用IP端口去访问，一般映射到外网的网站都会有一个域名，如：www.baidu.com, 输入的是域名，但实际上计算机还是要先转换成IP再来访问，转化的过程就叫做DNS解析</b>

- 为了帮助浏览器对某些域名进行解析，我们可以在页面的HTML的`link`标签中添加`dns-prefetch`会在浏览器空闲时对接下来可能访问的网站进行域名解析：
```html
<link rel="dns-prefetch" href="www.baidu.com">
```

### CDN部署与缓存
- 在网络上基站、信号强度、带宽总量是固定的，当大家都在远距离发送信息时，网络不得不做多次转发，并通过局域网来寻找对应的计算机，非常耗时间，甚至并发量很高的时候，网络可能会发生严重的拥堵，而CDN
实际上是将要访问的资源缓存到节点中，当我们访问资源时，实际上访问的这个网站是CDN节点
- 对于网站而言，任何的延迟或数据更新慢带来的影响都是比较大的，此时一个比较常见的做法就是将动态资源放在一个站点中，将静态资源，如图片、功能逻辑代码等放在另一个站点中，CDN只加速静态资源站点

### HTTP缓存
- HTTP缓存是浏览器缓存的一种，缓存运用的好，那么整站的性能可以得到质的飞跃，通过服务端读取数据的速度远比缓存中读取数据的速度慢，那么怎么看一个浏览器是否走HTTP缓存呢？
1. 根据头部信息判断是否命中强缓存，如果命中则直接加载缓存中的资源，不再将请求发送给服务器，状态码还是200，在size列会显示`(memory cache)`，即来自缓存
<img src="/imgs/cache01.png"/>
2. 如果没有命中强缓存，浏览器会将资源加载发送到服务器上，由服务器来判断浏览器本地缓存是否失效，如果没有失效，此时的服务器不返回资源，并将状态码置为304，浏览器继续从本地缓存中读取资源

3. 如果此时协商缓存仍未命中，那么服务器会将请求的这个资源从服务器完整的返回，那么浏览器怎么判断是否命中强缓存和协商缓存的？判断依据是什么？一般是放在头部信息中(请求头，响应头)
，浏览器读取头部信息后会进行解析判断，强缓存只要是由`Expires`和更高优先级的`Cache-control`来判断，具体请看网络篇

### 懒加载

## 其它
### 控制交互请求
- 浏览器并发条数限制在上面已有提及，在计算机资源里，能真正地对硬盘、内存等做分配的是系统(如：Linux、Windows等系统)，对于上层应用来说，只能从系统里分配一定的资源来完成它的功能，所以它的资源有限

#### 减少同后端交互请求数
- 浏览器的并发限制不仅对浏览器本身有好处，还对服务端有好处，对于服务端来说，某一时刻的并发量过大是一个非常重要的问题，而服务端并发总量想要提高，常见的做法有三种：1.加服务器、做集群，2.改变架构，如增加中间件分发、服务总线，3
.加快处理的速度，如数据库缓存、资源或模板等，但是加服务器非常耗钱，好的服务器很贵，做架构调整和缓存需要重构，非常耗时
- 那么前端开发时就要有意识的减少和后端的交互，如：做一下请求的合并、前端的缓存、资源的打包等，这些在有限资源的情况下做的调整相比于服务端的改动成本要小的多，主要的调整有：
1. 请求合并：如`CSS Sprite`，数据结构的调整(尽量不要采用数据量过于单一，不得不用多条请求完成某块前端区域展示的数据结构)
2. 前端缓存：`SessionStorage、LocalStorage、Cookie`等，`Cache`的合理设置也很重要，最简单的做法有缓存过期、失效时间的震荡错开
3. 资源打包：目前此类的工具已经有不少，如：`Gulp、Grunt、Webpack`等打包合并`JavaScript`，`JavaScript`是一种资源，由多个文件合并成一个，请求数自然而然就从多条合并成一条，和多个文件懒加载利弊参半
4. 其它如：将传统的轮询方式替换成`websocket`推送，也可以大大降低各种空的、无效的请求、降低服务端的压力

### 代理、中间件、请求分发
- 在后端与前端的中间做一个中间层(中间件)
做为请求分发的服务总线：做中间件的前提是应用的用户量确实很高，并发量很大，一台机器承受不住，要多台集群部署工作，假如只有一台服务器，那根本没必要，因为这个中间层只做一个请求分发，服务注册管理的作用只有一台，最终分发到的还是那台机器，与直接调用那台机器没有区别，甚至会因为多了一层反而变得更慢，中间层常见的一种实现方式是用`Nginx、Node.js`等中间层代理服务器

### 合理数据结构
- 合理的数据结构：后端易查、易存储，前端易取、易展示，可以最大限度的避免各种额外的数据结构，转化数据格式，可以缩短前端的`JavaScript`、后端线程的运行时长，提示应用的性能

#### 展示与处理
- 大部分清空下前端主要负责展示，后端负责数据处理，因为`JavaScript`是单线程的，而后端语言绝大多数都是多线程的
- 假设两个场景，一个数据在后端处理，一个数据在前端处理，数据在后端处理时很简单，可以开一个线程处理，处理完做一个返回，在这个过程中可以用其它线程来做其他事，而前端处理数据时，进程多数情况下是同步的，即`JavaScript`进程是被锁死的，只能等此处任务完成，才会继续执行其它任务，这显然不好。

#### 数据结构宜简不宜繁
- 现在主流的数据存储方式都是采用JSON轻量级的`key-value`数据格式，清晰且取值非常简单，但是即使是JSON格式的数据，也不建议用很深的层级，因为在取数据时，通常是通过遍历访问`key
`的形式访问数据，当层级过深时，只能一级一级往下访问，直到找到目标数据，这会在一定程度上影响数据读取的速度


<style>
#app .theme-default-content {
    max-width: 1200px;
}
</style>
